{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1257124,"sourceType":"datasetVersion","datasetId":723027}],"dockerImageVersionId":29869,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing few libraries\nimport os\nimport shutil\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport PIL\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T18:10:37.271417Z","iopub.execute_input":"2024-04-03T18:10:37.271730Z","iopub.status.idle":"2024-04-03T18:10:38.604769Z","shell.execute_reply.started":"2024-04-03T18:10:37.271696Z","shell.execute_reply":"2024-04-03T18:10:38.603920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I. Data Exploration","metadata":{}},{"cell_type":"code","source":"DATASET = \"../input/2750\"\n\nLABELS = os.listdir(DATASET)\nprint(LABELS)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:10:38.606958Z","iopub.execute_input":"2024-04-03T18:10:38.607253Z","iopub.status.idle":"2024-04-03T18:10:38.622304Z","shell.execute_reply.started":"2024-04-03T18:10:38.607216Z","shell.execute_reply":"2024-04-03T18:10:38.621673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot class distributions of whole dataset\ncounts = {}\n\nfor l in LABELS:\n    counts[l] = len(os.listdir(os.path.join(DATASET, l)))\n\n    \nplt.figure(figsize=(12, 6))\n\nplt.bar(range(len(counts)), list(counts.values()), align='center')\nplt.xticks(range(len(counts)), list(counts.keys()), fontsize=12, rotation=40)\nplt.xlabel('class label', fontsize=13)\nplt.ylabel('class size', fontsize=13)\nplt.title('EUROSAT Class Distribution', fontsize=15);","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:10:38.623333Z","iopub.execute_input":"2024-04-03T18:10:38.623570Z","iopub.status.idle":"2024-04-03T18:10:44.126231Z","shell.execute_reply.started":"2024-04-03T18:10:38.623543Z","shell.execute_reply":"2024-04-03T18:10:44.125296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_paths = [os.path.join(DATASET, l, l+'_1000.jpg') for l in LABELS]\n\nimg_paths = img_paths + [os.path.join(DATASET, l, l+'_2000.jpg') for l in LABELS]\n\ndef plot_sat_imgs(paths):\n    plt.figure(figsize=(15, 8))\n    for i in range(20):\n        plt.subplot(4, 5, i+1, xticks=[], yticks=[])\n        img = PIL.Image.open(paths[i], 'r')\n        plt.imshow(np.asarray(img))\n        plt.title(paths[i].split('/')[-2])\n\nplot_sat_imgs(img_paths)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:10:44.127742Z","iopub.execute_input":"2024-04-03T18:10:44.128061Z","iopub.status.idle":"2024-04-03T18:10:45.211677Z","shell.execute_reply.started":"2024-04-03T18:10:44.128023Z","shell.execute_reply":"2024-04-03T18:10:45.210882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# II. Preprocessing","metadata":{}},{"cell_type":"code","source":"import re\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom keras.preprocessing.image import ImageDataGenerator\n\nTRAIN_DIR = '../working/training'\nTEST_DIR = '../working/testing'\nBATCH_SIZE = 64\nNUM_CLASSES=len(LABELS)\nINPUT_SHAPE = (64, 64, 3)\nCLASS_MODE = 'categorical'\n\n# create training and testing directories\nfor path in (TRAIN_DIR, TEST_DIR):\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n# create class label subdirectories in train and test\nfor l in LABELS:\n    \n    if not os.path.exists(os.path.join(TRAIN_DIR, l)):\n        os.mkdir(os.path.join(TRAIN_DIR, l))\n\n    if not os.path.exists(os.path.join(TEST_DIR, l)):\n        os.mkdir(os.path.join(TEST_DIR, l))","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:10:45.214827Z","iopub.execute_input":"2024-04-03T18:10:45.215096Z","iopub.status.idle":"2024-04-03T18:10:51.489266Z","shell.execute_reply.started":"2024-04-03T18:10:45.215068Z","shell.execute_reply":"2024-04-03T18:10:51.488278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map each image path to their class label in 'data'\ndata = {}\n\nfor l in LABELS:\n    for img in os.listdir(DATASET+'/'+l):\n        data.update({os.path.join(DATASET, l, img): l})\n\nX = pd.Series(list(data.keys()))\ny = pd.get_dummies(pd.Series(data.values()))\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=69)\n\n# split the list of image paths\nfor train_idx, test_idx in split.split(X, y):\n    \n    train_paths = X[train_idx]\n    test_paths = X[test_idx]\n\n    # define a new path for each image depending on training or testing\n    new_train_paths = [re.sub('\\.\\.\\/input\\/2750', '../working/training', i) for i in train_paths]\n    new_test_paths = [re.sub('\\.\\.\\/input\\/2750', '../working/testing', i) for i in test_paths]\n\n    train_path_map = list((zip(train_paths, new_train_paths)))\n    test_path_map = list((zip(test_paths, new_test_paths)))\n    \n    # move the files\n    print(\"moving training files..\")\n    for i in tqdm(train_path_map):\n        if not os.path.exists(i[1]):\n            if not os.path.exists(re.sub('training', 'testing', i[1])):\n                shutil.copy(i[0], i[1])\n    \n    print(\"moving testing files..\")\n    for i in tqdm(test_path_map):\n        if not os.path.exists(i[1]):\n            if not os.path.exists(re.sub('training', 'testing', i[1])):\n                shutil.copy(i[0], i[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:10:51.491929Z","iopub.execute_input":"2024-04-03T18:10:51.492204Z","iopub.status.idle":"2024-04-03T18:15:12.057857Z","shell.execute_reply.started":"2024-04-03T18:10:51.492174Z","shell.execute_reply":"2024-04-03T18:15:12.056991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a ImageDataGenerator Instance which can be used for data augmentation\n\ntrain_gen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip = True\n#   validation_split=0.2\n)\n\ntrain_generator = train_gen.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    #subset='training',\n    color_mode='rgb',\n    shuffle=True,\n    seed=69\n)\n# The validation set is optional if we choose to do that\n\"\"\"\nvalid_generator = train_gen.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    subset='validation',    \n    color_mode='rgb',\n    shuffle=True,\n    seed=69\n)\n\"\"\"\n# test generator for evaluation purposes with no augmentations, just rescaling\ntest_gen = ImageDataGenerator(\n    rescale=1./255,\n)\n\ntest_generator = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:12.059681Z","iopub.execute_input":"2024-04-03T18:15:12.060021Z","iopub.status.idle":"2024-04-03T18:15:14.099183Z","shell.execute_reply.started":"2024-04-03T18:15:12.059980Z","shell.execute_reply":"2024-04-03T18:15:14.098381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_generator.class_indices)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:14.100367Z","iopub.execute_input":"2024-04-03T18:15:14.100588Z","iopub.status.idle":"2024-04-03T18:15:14.104582Z","shell.execute_reply.started":"2024-04-03T18:15:14.100563Z","shell.execute_reply":"2024-04-03T18:15:14.103905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('class_indices', train_generator.class_indices)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:14.105958Z","iopub.execute_input":"2024-04-03T18:15:14.106238Z","iopub.status.idle":"2024-04-03T18:15:14.116708Z","shell.execute_reply.started":"2024-04-03T18:15:14.106203Z","shell.execute_reply":"2024-04-03T18:15:14.116082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IV. Deep Learning For Image Classification\n\nDeep Learning has highly influenced the field of computer vision when Convolutional Neural Networks (CNN) models were used in tasks like image classification, object detection, facial recognition etc. As discussed by authors of EuroSAT paper many deep learning models outperform the traditional non deep learning methods by a large margin. \n\nIn this section we will train many state of the art architectures which performed well on the ILSVRC challenge. The authors achieved an accuracy of 98.57% using a fine tuned ResNet50 model. Here, I will try to employ a similar strategy for training the models where initially the CNN part of the model will be frozen with imagenet weights and dense layers will be trained with a high learning rate of 0.01 and later we will train the whole model end-to-end i.e. fine tune by keeping a small learning rate between 0.001 to 0.0001\n\nBefore we start here is a list of CNN models which we will train:\n1. ResNet50\n2. ResNet50V2\n3. ResNet152V2\n3. VGG16\n4. VGG19\n\nIn future\n5. InceptionV3\n6. Xception","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\n\nfrom keras.applications import VGG16, VGG19\nfrom keras.applications import ResNet50, ResNet50V2, ResNet152V2\nfrom keras.applications import InceptionV3, Xception\n\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, fbeta_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:14.117804Z","iopub.execute_input":"2024-04-03T18:15:14.118095Z","iopub.status.idle":"2024-04-03T18:15:14.127732Z","shell.execute_reply.started":"2024-04-03T18:15:14.118067Z","shell.execute_reply":"2024-04-03T18:15:14.126970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")    \n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)\n    \ntf.config.set_soft_device_placement(True)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:14.128661Z","iopub.execute_input":"2024-04-03T18:15:14.129040Z","iopub.status.idle":"2024-04-03T18:15:16.489745Z","shell.execute_reply.started":"2024-04-03T18:15:14.128992Z","shell.execute_reply":"2024-04-03T18:15:16.488843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note that for different CNN models we are using different setup of dense layers\ndef compile_model(cnn_base, input_shape, n_classes, optimizer, fine_tune=None):\n    \n    if (cnn_base == 'ResNet50') or (cnn_base == 'ResNet50V2') or (cnn_base == 'ResNet152V2'):\n        if cnn_base == 'ResNet50':\n            conv_base = ResNet50(include_top=False,\n                                 weights='imagenet', \n                                 input_shape=input_shape)\n        elif cnn_base == 'ResNet50V2':\n            conv_base = ResNet50V2(include_top=False,\n                                 weights='imagenet', \n                                 input_shape=input_shape)\n        else:\n            conv_base = ResNet152V2(include_top=False,\n                                 weights='imagenet', \n                                 input_shape=input_shape)\n        top_model = conv_base.output\n        top_model = Flatten()(top_model)\n        top_model = Dense(2048, activation='relu')(top_model)\n        top_model = Dropout(0.2)(top_model)\n       \n    \n    elif (cnn_base == 'VGG16') or (cnn_base == 'VGG19'):\n        if cnn_base == 'VGG16':\n            conv_base = VGG16(include_top=False,\n                              weights='imagenet', \n                              input_shape=input_shape)\n        else:\n            conv_base = VGG19(include_top=False,\n                              weights='imagenet', \n                              input_shape=input_shape)\n        top_model = conv_base.output\n        top_model = Flatten()(top_model)\n        top_model = Dense(2048, activation='relu')(top_model)\n        top_model = Dropout(0.2)(top_model)\n        top_model = Dense(2048, activation='relu')(top_model)\n        top_model = Dropout(0.2)(top_model)\n    \n    \n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    model = Model(inputs=conv_base.input, outputs=output_layer)\n        \n    if type(fine_tune) == int:\n        for layer in conv_base.layers[fine_tune:]:\n            layer.trainable = True\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                 metrics=['categorical_accuracy'])\n    \n    return model\n\ndef plot_history(history):\n       \n    acc = history.history['categorical_accuracy']\n    val_acc = history.history['val_categorical_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.show();\n\ndef display_results(y_true, y_preds, class_labels):\n    \n    results = pd.DataFrame(precision_recall_fscore_support(y_true, y_preds),\n                          columns=class_labels).T\n    results.rename(columns={0: 'Precision',\n                           1: 'Recall',\n                           2: 'F-Score',\n                           3: 'Support'}, inplace=True)\n    \n    conf_mat = pd.DataFrame(confusion_matrix(y_true, y_preds), \n                            columns=class_labels,\n                            index=class_labels)    \n    f2 = fbeta_score(y_true, y_preds, beta=2, average='micro')\n    accuracy = accuracy_score(y_true, y_preds)\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"Global F2 Score: {f2}\")    \n    return results, conf_mat\n\ndef plot_predictions(y_true, y_preds, test_generator, class_indices):\n\n    fig = plt.figure(figsize=(20, 10))\n    for i, idx in enumerate(np.random.choice(test_generator.samples, size=20, replace=False)):\n        ax = fig.add_subplot(4, 5, i + 1, xticks=[], yticks=[])\n        ax.imshow(np.squeeze(test_generator[idx]))\n        pred_idx = np.argmax(y_preds[idx])\n        true_idx = y_true[idx]\n                \n        plt.tight_layout()\n        ax.set_title(\"{}\\n({})\".format(class_indices[pred_idx], class_indices[true_idx]),\n                     color=(\"green\" if pred_idx == true_idx else \"red\"))    ","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:16.491461Z","iopub.execute_input":"2024-04-03T18:15:16.491821Z","iopub.status.idle":"2024-04-03T18:15:16.527187Z","shell.execute_reply.started":"2024-04-03T18:15:16.491748Z","shell.execute_reply":"2024-04-03T18:15:16.526240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_STEPS = train_generator.samples//BATCH_SIZE\nN_VAL_STEPS = test_generator.samples//BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=5,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:16.528600Z","iopub.execute_input":"2024-04-03T18:15:16.528940Z","iopub.status.idle":"2024-04-03T18:15:16.542739Z","shell.execute_reply.started":"2024-04-03T18:15:16.528904Z","shell.execute_reply":"2024-04-03T18:15:16.542138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1 ResNet50 Model","metadata":{}},{"cell_type":"code","source":"resnet50_model = compile_model('ResNet50', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\nresnet50_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:16.544085Z","iopub.execute_input":"2024-04-03T18:15:16.544379Z","iopub.status.idle":"2024-04-03T18:15:21.323035Z","shell.execute_reply.started":"2024-04-03T18:15:16.544343Z","shell.execute_reply":"2024-04-03T18:15:21.322268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator.reset()\ntest_generator.reset()\n\nN_STEPS = train_generator.samples//BATCH_SIZE\nN_VAL_STEPS = test_generator.samples//BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=5,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:21.324267Z","iopub.execute_input":"2024-04-03T18:15:21.324502Z","iopub.status.idle":"2024-04-03T18:15:21.331360Z","shell.execute_reply.started":"2024-04-03T18:15:21.324475Z","shell.execute_reply":"2024-04-03T18:15:21.330559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First Pretraining the dense layer\nresnet50_history = resnet50_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=50,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:15:21.332569Z","iopub.execute_input":"2024-04-03T18:15:21.332829Z","iopub.status.idle":"2024-04-03T18:19:06.539142Z","shell.execute_reply.started":"2024-04-03T18:15:21.332801Z","shell.execute_reply":"2024-04-03T18:19:06.538365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# re-train whole network end2end \nresnet50_model = compile_model('ResNet50', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n\nresnet50_model.load_weights('../working/model.weights.best.hdf5')\n\ntrain_generator.reset()\ntest_generator.reset()\n\nresnet50_history = resnet50_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint, reduce_lr],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:19:06.541013Z","iopub.execute_input":"2024-04-03T18:19:06.541242Z","iopub.status.idle":"2024-04-03T18:29:19.286759Z","shell.execute_reply.started":"2024-04-03T18:19:06.541212Z","shell.execute_reply":"2024-04-03T18:29:19.285559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(resnet50_history)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:29:19.288506Z","iopub.execute_input":"2024-04-03T18:29:19.288743Z","iopub.status.idle":"2024-04-03T18:29:19.634101Z","shell.execute_reply.started":"2024-04-03T18:29:19.288717Z","shell.execute_reply":"2024-04-03T18:29:19.633404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet50_model.load_weights('../working/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator_new = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)\n\npredictions = resnet50_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator_new.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:29:19.635418Z","iopub.execute_input":"2024-04-03T18:29:19.635637Z","iopub.status.idle":"2024-04-03T18:30:35.723080Z","shell.execute_reply.started":"2024-04-03T18:29:19.635612Z","shell.execute_reply":"2024-04-03T18:30:35.722203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and the weights\nresnet50_model.save('../working/ResNet50_eurosat.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:30:35.724450Z","iopub.execute_input":"2024-04-03T18:30:35.724676Z","iopub.status.idle":"2024-04-03T18:30:37.146947Z","shell.execute_reply.started":"2024-04-03T18:30:35.724651Z","shell.execute_reply":"2024-04-03T18:30:37.146006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 VGG16 Model","metadata":{}},{"cell_type":"code","source":"vgg16_model = compile_model('VGG16', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\nvgg16_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:30:37.148251Z","iopub.execute_input":"2024-04-03T18:30:37.148505Z","iopub.status.idle":"2024-04-03T18:30:39.267521Z","shell.execute_reply.started":"2024-04-03T18:30:37.148476Z","shell.execute_reply":"2024-04-03T18:30:39.266730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator.reset()\ntest_generator.reset()\n\nN_STEPS = train_generator.samples//BATCH_SIZE\nN_VAL_STEPS = test_generator.samples//BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='../working/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=5,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:30:39.269242Z","iopub.execute_input":"2024-04-03T18:30:39.269570Z","iopub.status.idle":"2024-04-03T18:30:39.277083Z","shell.execute_reply.started":"2024-04-03T18:30:39.269527Z","shell.execute_reply":"2024-04-03T18:30:39.276248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator.reset()\n# First Pretraining the dense layer\nvgg16_history = vgg16_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=50,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:30:39.278874Z","iopub.execute_input":"2024-04-03T18:30:39.279168Z","iopub.status.idle":"2024-04-03T18:40:35.050327Z","shell.execute_reply.started":"2024-04-03T18:30:39.279132Z","shell.execute_reply":"2024-04-03T18:40:35.048829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# re-train whole network end2end \nvgg16_model = compile_model('VGG16', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n\nvgg16_model.load_weights('../working/model.weights.best.hdf5')\n\ntrain_generator.reset()\ntest_generator.reset()\n\nvgg16_history = vgg16_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint, reduce_lr],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:40:35.052468Z","iopub.execute_input":"2024-04-03T18:40:35.052784Z","iopub.status.idle":"2024-04-03T18:53:22.533257Z","shell.execute_reply.started":"2024-04-03T18:40:35.052743Z","shell.execute_reply":"2024-04-03T18:53:22.532004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(vgg16_history)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:53:22.535018Z","iopub.execute_input":"2024-04-03T18:53:22.535278Z","iopub.status.idle":"2024-04-03T18:53:22.905093Z","shell.execute_reply.started":"2024-04-03T18:53:22.535247Z","shell.execute_reply":"2024-04-03T18:53:22.903759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16_model.load_weights('../working/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator_new = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)\n\npredictions = vgg16_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator_new.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:53:22.906928Z","iopub.execute_input":"2024-04-03T18:53:22.907179Z","iopub.status.idle":"2024-04-03T18:53:44.341541Z","shell.execute_reply.started":"2024-04-03T18:53:22.907149Z","shell.execute_reply":"2024-04-03T18:53:44.340701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and the weights\nvgg16_model.save('../working/vgg16_eurosat.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:53:44.343061Z","iopub.execute_input":"2024-04-03T18:53:44.343367Z","iopub.status.idle":"2024-04-03T18:53:44.839524Z","shell.execute_reply.started":"2024-04-03T18:53:44.343327Z","shell.execute_reply":"2024-04-03T18:53:44.838531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_predictions(true_classes, predictions, test_generator_new, class_indices)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T18:53:44.841037Z","iopub.execute_input":"2024-04-03T18:53:44.841310Z","iopub.status.idle":"2024-04-03T18:53:47.014373Z","shell.execute_reply.started":"2024-04-03T18:53:44.841278Z","shell.execute_reply":"2024-04-03T18:53:47.013408Z"},"trusted":true},"execution_count":null,"outputs":[]}]}